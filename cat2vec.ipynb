{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "d0b855f0-f14e-5952-b118-4f8e1cbd572c",
        "openai_ephemeral_user_id": "c8a55c9d-9f4a-50e0-8c13-e7f8bf2dafd2",
        "openai_subdivision1_iso_code": "GB-ENG"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "identifier": "legacy",
      "language": "python",
      "language_version": "3.9",
      "name": "python3"
    },
    "selected_hardware_size": "small"
  },
  "cells": [
    {
      "id": "1ca07e74-d949-44e2-bc51-886a707ae560",
      "cell_type": "markdown",
      "source": "# cat2vec\n\n## Aim of the Notebook\nThe main idea of this notebook is to develop a snippet of code to transform categorical features into embeddings using the Gensim Word2Vec library. Below is an initial draft of the methods that will be used:\n\n### Method 1: `apply_w2v`\nThis method takes sentences, a Word2Vec model, and the number of features as input. It returns the average word vectors for each sentence.\n\n```python\ndef apply_w2v(sentences, model, num_features):\n    # ...\n```\n\n### Method 2: `gen_cat2vec_sentences`\nThis method takes a DataFrame and returns a list of sentences, where each sentence is a list of categories.\n\n```python\ndef gen_cat2vec_sentences(df):\n    # ...\n```\n\n### Method 3: `fit_cat2vec_model`\nThis method takes a DataFrame, the number of features, and the window size for the Word2Vec model. It returns a trained Word2Vec model.\n\n```python\ndef fit_cat2vec_model(df, n_cat2vec_features, n_cat2vec_window):\n    # ...\n```\n\nWe will load a toy classification dataset and create an example using a neural network model.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "b1167386-eede-44ee-9c69-7804ab83742f",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "f20088fd-dbe2-4978-a4bf-3685987584b2"
        },
        "ExecuteTime": {
          "end_time": "2023-10-28T08:17:25.397222+00:00",
          "start_time": "2023-10-28T08:17:21.320851+00:00"
        }
      },
      "execution_count": null,
      "source": "!pip install gensim",
      "outputs": []
    },
    {
      "id": "94db5e96-aab7-4fb4-950c-d948e9e2bc35",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "2ee8779d-5b0f-442c-88f5-45b0c518f2a2"
        },
        "ExecuteTime": {
          "end_time": "2023-10-28T08:16:15.014762+00:00",
          "start_time": "2023-10-28T08:16:12.724186+00:00"
        },
        "datalink": {
          "887c0022-586e-4fbd-bc8e-fde348990326": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 5,
              "orig_num_rows": 5,
              "orig_size_bytes": 240,
              "truncated_num_cols": 5,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 240,
              "truncated_string_columns": []
            },
            "display_id": "887c0022-586e-4fbd-bc8e-fde348990326",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-10-28T08:14:26.323001",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_dbfd53b6d77a4546a33acac07795b4ad"
          },
          "4bd0bc45-8a75-468d-ba14-edab0445a592": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 14,
              "orig_num_rows": 5,
              "orig_size_bytes": 875,
              "truncated_num_cols": 14,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 875,
              "truncated_string_columns": []
            },
            "display_id": "4bd0bc45-8a75-468d-ba14-edab0445a592",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-10-28T08:16:14.848574",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_869c01e672d4412984018b171387f972"
          }
        },
        "dx": {
          "fieldMetadata": {},
          "simpleTable": true,
          "updated": 1698480871741,
          "views": []
        }
      },
      "execution_count": null,
      "source": "from sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# Load the Titanic dataset\ndata = fetch_openml('titanic', version=1, as_frame=True)\ndf = data['data']\ndf['target'] = data['target']\n\n# Split the dataset into training and test sets\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n\n# Show the first few rows of the training data\ndf_train.head()",
      "outputs": []
    },
    {
      "id": "fb12b19d-c4e3-4705-9959-12e5a13fee88",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "6d8ea28c-594c-4c70-814e-c1cee27192fa"
        },
        "ExecuteTime": {
          "end_time": "2023-10-28T08:17:33.587386+00:00",
          "start_time": "2023-10-28T08:17:33.310186+00:00"
        }
      },
      "execution_count": null,
      "source": "from gensim.models import Word2Vec\nfrom random import shuffle\nimport numpy as np\n\ndef apply_w2v(sentences, model, num_features):\n    def _average_word_vectors(words, model, vocabulary, num_features):\n        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n        n_words = 0.\n        for word in words:\n            if word in vocabulary:\n                n_words = n_words + 1.\n                feature_vector = np.add(feature_vector, model.wv[word])\n        if n_words:\n            feature_vector = np.divide(feature_vector, n_words)\n        return feature_vector\n\n    vocab = set(model.wv.index_to_key)\n    feats = [_average_word_vectors(s, model, vocab, num_features) for s in sentences]\n    return np.array(feats)\n\ndef gen_cat2vec_sentences(df):\n    X_w2v = df.copy(deep=True)\n    names = list(X_w2v.columns.values)\n    for c in names:\n        X_w2v[c] = X_w2v[c].fillna('unknow').astype('category')\n        X_w2v[c].cat.categories = [\"%s %s\" % (c,g) for g in X_w2v[c].cat.categories]\n    X_w2v = X_w2v.values.tolist()\n    return X_w2v\n\ndef fit_cat2vec_model(df, n_cat2vec_features, n_cat2vec_window):\n    X_w2v = gen_cat2vec_sentences(df.sample(frac=0.6))\n    for i in X_w2v:\n        shuffle(i)\n    model = Word2Vec(X_w2v, vector_size=n_cat2vec_features, window=n_cat2vec_window)\n    return model",
      "outputs": []
    },
    {
      "id": "b8be1211-9283-489c-998b-f8b4dee3ff2c",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "397d6f32-da35-4ff0-9b67-0fe1c2ef5030"
        },
        "ExecuteTime": {
          "end_time": "2023-10-28T08:17:39.764584+00:00",
          "start_time": "2023-10-28T08:17:39.552863+00:00"
        }
      },
      "execution_count": null,
      "source": "# Identify categorical columns\ncategorical_cols = df_train.select_dtypes(include=['object']).columns.tolist()\n\n# Use only the categorical features for cat2vec\ndf_train_cat = df_train[categorical_cols]\ndf_test_cat = df_test[categorical_cols]\n\n# Fit the cat2vec model\nn_cat2vec_features = 50\nn_cat2vec_window = 5\nmodel = fit_cat2vec_model(df_train_cat, n_cat2vec_features, n_cat2vec_window)\n\n# Generate cat2vec sentences for training and test sets\nsentences_train = gen_cat2vec_sentences(df_train_cat)\nsentences_test = gen_cat2vec_sentences(df_test_cat)\n\n# Apply Word2Vec model to generate feature vectors\ntrain_features = apply_w2v(sentences_train, model, n_cat2vec_features)\ntest_features = apply_w2v(sentences_test, model, n_cat2vec_features)\n\n# Show the shape of the generated feature vectors\ntrain_features.shape, test_features.shape",
      "outputs": []
    },
    {
      "id": "0fc43c68-26c9-41fa-a30e-5e23a3271ddc",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "",
      "outputs": []
    },
    {
      "id": "dda3c4d7-1b50-4a0a-b2a3-c7b5e51f9ddb",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "# Create DataFrame from the transformed features\ndf_train_features = pd.DataFrame(train_features, columns=[f'cat2vec_{i}' for i in range(n_cat2vec_features)])\ndf_test_features = pd.DataFrame(test_features, columns=[f'cat2vec_{i}' for i in range(n_cat2vec_features)])\n\n# Reset index for concatenation\ndf_train.reset_index(drop=True, inplace=True)\ndf_test.reset_index(drop=True, inplace=True)\ndf_train_features.reset_index(drop=True, inplace=True)\ndf_test_features.reset_index(drop=True, inplace=True)\n\n# Concatenate the transformed features back to the original DataFrame\ndf_train_transformed = pd.concat([df_train, df_train_features], axis=1)\ndf_test_transformed = pd.concat([df_test, df_test_features], axis=1)\n\n# Show the first few rows of the transformed training data\ndf_train_transformed.head()",
      "outputs": []
    },
    {
      "id": "e1d2e8d1-a8f9-424a-af49-10322e7099a7",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "# Create DataFrame from the transformed features\ndf_train_features = pd.DataFrame(train_features, columns=[f'cat2vec_{i}' for i in range(n_cat2vec_features)])\ndf_test_features = pd.DataFrame(test_features, columns=[f'cat2vec_{i}' for i in range(n_cat2vec_features)])\n\n# Reset index for concatenation\ndf_train.reset_index(drop=True, inplace=True)\ndf_test.reset_index(drop=True, inplace=True)\ndf_train_features.reset_index(drop=True, inplace=True)\ndf_test_features.reset_index(drop=True, inplace=True)\n\n# Concatenate the transformed features back to the original DataFrame\ndf_train_transformed = pd.concat([df_train, df_train_features], axis=1)\ndf_test_transformed = pd.concat([df_test, df_test_features], axis=1)\n\n# Show the first few rows of the transformed training data\ndf_train_transformed.head()",
      "outputs": []
    }
  ]
}