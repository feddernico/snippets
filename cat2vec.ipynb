{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "d0b855f0-f14e-5952-b118-4f8e1cbd572c",
        "openai_ephemeral_user_id": "c8a55c9d-9f4a-50e0-8c13-e7f8bf2dafd2",
        "openai_subdivision1_iso_code": "GB-ENG"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "identifier": "legacy",
      "language": "python",
      "language_version": "3.9",
      "name": "python3"
    },
    "selected_hardware_size": "small"
  },
  "cells": [
    {
      "id": "18d2fd53-5de7-41e7-8e5d-2c81b3b3c409",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "",
      "outputs": []
    },
    {
      "id": "1ca07e74-d949-44e2-bc51-886a707ae560",
      "cell_type": "markdown",
      "source": "# cat2vec\n\n## Aim of the Notebook\nThe main idea of this notebook is to develop a snippet of code to transform categorical features into embeddings using the Gensim Word2Vec library. Below is an initial draft of the methods that will be used:\n\n### Method 1: `apply_w2v`\nThis method takes sentences, a Word2Vec model, and the number of features as input. It returns the average word vectors for each sentence.\n\n```python\ndef apply_w2v(sentences, model, num_features):\n    # ...\n```\n\n### Method 2: `gen_cat2vec_sentences`\nThis method takes a DataFrame and returns a list of sentences, where each sentence is a list of categories.\n\n```python\ndef gen_cat2vec_sentences(df):\n    # ...\n```\n\n### Method 3: `fit_cat2vec_model`\nThis method takes a DataFrame, the number of features, and the window size for the Word2Vec model. It returns a trained Word2Vec model.\n\n```python\ndef fit_cat2vec_model(df, n_cat2vec_features, n_cat2vec_window):\n    # ...\n```\n\nWe will load a toy classification dataset and create an example using a neural network model.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "94db5e96-aab7-4fb4-950c-d948e9e2bc35",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "c4f6c260-9896-4bab-b2f7-a14dd6b0e177"
        },
        "ExecuteTime": {
          "end_time": "2023-10-28T08:14:26.485620+00:00",
          "start_time": "2023-10-28T08:14:26.064755+00:00"
        },
        "datalink": {
          "887c0022-586e-4fbd-bc8e-fde348990326": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 5,
              "orig_num_rows": 5,
              "orig_size_bytes": 240,
              "truncated_num_cols": 5,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 240,
              "truncated_string_columns": []
            },
            "display_id": "887c0022-586e-4fbd-bc8e-fde348990326",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-10-28T08:14:26.323001",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_dbfd53b6d77a4546a33acac07795b4ad"
          }
        },
        "dx": {
          "fieldMetadata": {},
          "simpleTable": true,
          "updated": 1698480871741,
          "views": []
        }
      },
      "execution_count": null,
      "source": "from sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# Load the Titanic dataset\ndata = fetch_openml('titanic', version=1, as_frame=True)\ndf = data['data']\ndf['target'] = data['target']\n\n# Split the dataset into training and test sets\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n\n# Show the first few rows of the training data\ndf_train.head()",
      "outputs": []
    },
    {
      "id": "fb12b19d-c4e3-4705-9959-12e5a13fee88",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "40fe3ebd-27fb-4d36-ae33-205a4e327765"
        },
        "ExecuteTime": {
          "end_time": "2023-10-28T08:14:41.911178+00:00",
          "start_time": "2023-10-28T08:14:41.540295+00:00"
        }
      },
      "execution_count": null,
      "source": "from gensim.models import Word2Vec\nfrom random import shuffle\nimport numpy as np\n\ndef apply_w2v(sentences, model, num_features):\n    def _average_word_vectors(words, model, vocabulary, num_features):\n        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n        n_words = 0.\n        for word in words:\n            if word in vocabulary:\n                n_words = n_words + 1.\n                feature_vector = np.add(feature_vector, model.wv[word])\n        if n_words:\n            feature_vector = np.divide(feature_vector, n_words)\n        return feature_vector\n\n    vocab = set(model.wv.index_to_key)\n    feats = [_average_word_vectors(s, model, vocab, num_features) for s in sentences]\n    return np.array(feats)\n\ndef gen_cat2vec_sentences(df):\n    X_w2v = df.copy(deep=True)\n    names = list(X_w2v.columns.values)\n    for c in names:\n        X_w2v[c] = X_w2v[c].fillna('unknow').astype('category')\n        X_w2v[c].cat.categories = [\"%s %s\" % (c,g) for g in X_w2v[c].cat.categories]\n    X_w2v = X_w2v.values.tolist()\n    return X_w2v\n\ndef fit_cat2vec_model(df, n_cat2vec_features, n_cat2vec_window):\n    X_w2v = gen_cat2vec_sentences(df.sample(frac=0.6))\n    for i in X_w2v:\n        shuffle(i)\n    model = Word2Vec(X_w2v, vector_size=n_cat2vec_features, window=n_cat2vec_window)\n    return model",
      "outputs": []
    },
    {
      "id": "be9f730e-02e5-4818-ab8c-394bc96f5bf3",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "",
      "outputs": []
    }
  ]
}