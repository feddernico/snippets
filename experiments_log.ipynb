{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "d0b855f0-f14e-5952-b118-4f8e1cbd572c",
        "openai_ephemeral_user_id": "c8a55c9d-9f4a-50e0-8c13-e7f8bf2dafd2",
        "openai_subdivision1_iso_code": "GB-ENG"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "identifier": "legacy",
      "language": "python",
      "language_version": "3.9",
      "name": "python3"
    },
    "selected_hardware_size": "small"
  },
  "cells": [
    {
      "id": "80591fa0-813f-49fc-96fd-74cf774e2b74",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "",
      "outputs": []
    },
    {
      "id": "b4717f29-c555-4200-b163-8fa906bec0ba",
      "cell_type": "markdown",
      "source": "# Experiments Log\n\n## Notebook Description\nThe aim of this notebook is to demonstrate how to gather the hyperparameters and the performance of a model, measured in terms of:\n- Accuracy\n- Precision\n- Recall\n- F1 Score\n- True Positives (TP)\n- False Positives (FP)\n- True Negatives (TN)\n- False Negatives (FN)\n\nThese metrics will be logged together in a dictionary and then saved in a CSV file. This approach allows us to easily retrieve which hyperparameters are producing the highest performing model with the current dataset.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    }
  ]
}