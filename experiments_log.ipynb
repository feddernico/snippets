{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "d0b855f0-f14e-5952-b118-4f8e1cbd572c",
        "openai_ephemeral_user_id": "c8a55c9d-9f4a-50e0-8c13-e7f8bf2dafd2",
        "openai_subdivision1_iso_code": "GB-ENG"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "identifier": "legacy",
      "language": "python",
      "language_version": "3.9",
      "name": "python3"
    },
    "selected_hardware_size": "small"
  },
  "cells": [
    {
      "id": "b4717f29-c555-4200-b163-8fa906bec0ba",
      "cell_type": "markdown",
      "source": "# Experiments Log\n\n## Notebook Description\nThe aim of this notebook is to demonstrate how to gather the hyperparameters and the performance of a model, measured in terms of:\n- Accuracy\n- Precision\n- Recall\n- F1 Score\n- True Positives (TP)\n- False Positives (FP)\n- True Negatives (TN)\n- False Negatives (FN)\n\nThese metrics will be logged together in a dictionary and then saved in a CSV file. This approach allows us to easily retrieve which hyperparameters are producing the highest performing model with the current dataset.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "2a51033b-d424-4c13-97ee-e080fac50624",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "96b30c8e-2793-438c-befd-ed24ccf8c14e"
        },
        "ExecuteTime": {
          "end_time": "2023-10-28T07:38:00.862792+00:00",
          "start_time": "2023-10-28T07:38:00.482516+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n# Load the Iris dataset\ndata = load_iris()\nX, y = data.data, data.target\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Show the shape of the data\nX_train.shape, X_test.shape, y_train.shape, y_test.shape",
      "outputs": []
    },
    {
      "id": "59bfbe6f-99c6-41ef-ac04-93532bd35583",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "824baa65-d313-47b7-a1dc-4c5c7e54c04a"
        },
        "ExecuteTime": {
          "end_time": "2023-10-28T07:39:24.120836+00:00",
          "start_time": "2023-10-28T07:39:23.876189+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport pandas as pd\n\n# Initialize an empty dictionary to store experiment logs\nexperiment_logs = {}\n\n# Define hyperparameters\nn_estimators = 100\nmax_depth = 5\n\n# Initialize and train the model\nclf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\nclf.fit(X_train, y_train)\n\n# Make predictions\ny_pred = clf.predict(X_test)\n\n# Calculate metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted')\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Log the experiment\nexperiment_logs['experiment_1'] = {\n    'n_estimators': n_estimators,\n    'max_depth': max_depth,\n    'accuracy': accuracy,\n    'precision': precision,\n    'recall': recall,\n    'f1_score': f1,\n    'confusion_matrix': conf_matrix.tolist()\n}\n\n# Convert logs to DataFrame and save as CSV\npd.DataFrame.from_dict(experiment_logs, orient='index').to_csv('experiment_logs.csv')\n\n# Show the experiment logs\nexperiment_logs",
      "outputs": []
    }
  ]
}